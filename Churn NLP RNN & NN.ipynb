{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image, ImageFont, ImageDraw\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#Text Analysis\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "######################################################Initial Packages########################################################\n",
    "#Basic Operating System Stuff\n",
    "import sklearn\n",
    "import os\n",
    "import gc #garbage collector\n",
    "import random #random seed generator\n",
    "import sys\n",
    "import csv\n",
    "import subprocess\n",
    "\n",
    "#Basic dataframe, array, and math stuff\n",
    "import pandas as pd #data frame\n",
    "import math #math functions\n",
    "import numpy as np    #numerical package\n",
    "\n",
    "#Sci-kit Learn\n",
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "from sklearn.metrics import classification_report as cr\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "\n",
    "#Tensorflow\n",
    "import tensorflow as tf #backend for keras\n",
    "from tensorflow.python.client import device_lib #to  see if my GPU is alive!\n",
    "import tensorflow.keras #keras\n",
    "from tensorflow.keras import Sequential,Input,Model #pull in the sequential, input layers and a model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Embedding, Bidirectional, Dense, LSTM, Concatenate, concatenate #pull in the dense, dropout, and flatten layers\n",
    "from tensorflow.keras.layers import Add, Activation, ZeroPadding2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.applications import EfficientNetB6#MobileNetV2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint # ReduceLROnPlateau, use for early stopping and reduction on level-out\n",
    "from tensorflow.keras import utils #Need utilities for the layers\n",
    "from tensorflow.keras.utils import get_file, plot_model #To load certain files & plot models\n",
    "import tensorflow.keras.backend as K #To write our own metrics and loss functions\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #RNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences #RNN\n",
    "\n",
    "#Graphing\n",
    "import matplotlib #image save\n",
    "import matplotlib.pyplot as plt #plotting\n",
    "from matplotlib.pyplot import imshow #Show images\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "#Text Analysis\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "import re\n",
    "import spacy\n",
    "from spacymoji import Emoji\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "from num2words import num2words\n",
    "\n",
    "\n",
    "\n",
    "#Directory\n",
    "\n",
    "os.chdir(\"c:/users/lfult/documents/churnplay/\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Verify GPU recognition\n",
    "print(device_lib.list_local_devices()) #Let's see if Python recognizes my GPU, shall we?\n",
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################Memory Mgt / Directory Load###################################################\n",
    "def reset_keras():\n",
    "    tf.keras.backend.clear_session #This clears the GPU session\n",
    "    gc.collect()\n",
    "reset_keras()            \n",
    "################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=pd.read_csv('modtrain.csv', dtype='str')\n",
    "testdf=pd.read_csv('modtest.csv', dtype='str')\n",
    "traindf.head()\n",
    "nlpdata=pd.DataFrame(pd.concat([traindf['Concatenate'], testdf['Concatenate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from num2words import num2words\n",
    "from string import punctuation\n",
    "from emot.emo_unicode import UNICODE_EMOJI \n",
    "from emot.emo_unicode import EMOTICONS_EMO # For EMOTICONS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from bs4 import BeautifulSoup\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "\n",
    "def clean_html(text):\n",
    "  CLEANR = re.compile('<.*?>') \n",
    "  text = re.sub(CLEANR, '', text)\n",
    "  return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return [word for word in word_tokenize(text) if not word in stopwords]\n",
    "\n",
    "def strip_punctuation(text):\n",
    "    return ''.join(c for c in text if c not in punctuation)+' '    \n",
    "\n",
    "def to_number(text):    \n",
    "    return(re.sub(r\"(\\d+)\", lambda x: num2words(int(x.group(0))), text))\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Apply PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlpdata['Concatenate']=nlpdata['Concatenate'].str.lower()\n",
    "nlpdata['Concatenate']=nlpdata['Concatenate'].astype(str).apply(clean_html)\n",
    "nlpdata['Concatenate']=nlpdata['Concatenate'].astype(str).apply(remove_stopwords)\n",
    "nlpdata['Concatenate']=nlpdata['Concatenate'].astype(str).apply(strip_punctuation)\n",
    "nlpdata['Concatenate']=nlpdata['Concatenate'].astype(str).apply(to_number)\n",
    "nlpdata['Concatenate']=nlpdata['Concatenate'].astype(str).apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split back to Training / Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=nlpdata.iloc[0:175036,]\n",
    "test=nlpdata.iloc[175036:285059,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('trainNLP.csv', index=False)\n",
    "test.to_csv('testNLP.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"trainNLP.csv\")\n",
    "test=pd.read_csv(\"testNLP.csv\")\n",
    "train['label']=traindf['Exited']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_keras()\n",
    "mybatch=64   #set batch size\n",
    "embedding_size=512 #set out size for embedding\n",
    "num_epochs=4 #set the number of epochs..\n",
    "num_words = 50000     #number of words to tokenize, 50000 was start\n",
    "tf.random.set_seed(1234) #set random number seed for tensorflow, python, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=pd.read_csv('modtrain.csv')\n",
    "test2=pd.read_csv('modtest.csv')\n",
    "\n",
    "x_train1=train['Concatenate'] \n",
    "x_test1=test['Concatenate']      \n",
    "\n",
    "x_train2=train2.iloc[:,3:12]\n",
    "x_test2=test2.iloc[:, 3:12]       \n",
    "\n",
    "y_train=train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2=pd.get_dummies(x_train2)\n",
    "x_test2=pd.get_dummies(x_test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Max Scale for Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=mms() #declare min max scaler\n",
    "x_train2=transform.fit_transform(x_train2)   #fit transform training data\n",
    "x_test2=transform.transform(x_test2)                                                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oov_token = '<UNK>'   #out of vocabulary replacement\n",
    "pad_type = 'post'     #padding type\n",
    "trunc_type = 'post'   #truncation type\n",
    "\n",
    "tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "\n",
    "tokenizer.fit_on_texts(x_train1.astype('str'))\n",
    "#tokenizer.fit_on_texts(x_train2.astype('str'))\n",
    "\n",
    "# Encode training data sentences into sequences\n",
    "train_sequences1 = tokenizer.texts_to_sequences(x_train1.astype('str')) \n",
    "\n",
    "# Get max training sequence length\n",
    "maxlen1 = max([len(x) for x in train_sequences1])\n",
    "\n",
    "# Pad the training sequences to the maximum sentence length\n",
    "x_train1 = pad_sequences(train_sequences1, padding=pad_type, truncating=trunc_type, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode test data sentences into sequences\n",
    "test_sequences1 = tokenizer.texts_to_sequences(x_test1.astype('str'))\n",
    "\n",
    "# Pad the training sequences to the maximum sentence length of training set\n",
    "x_test1 = pad_sequences(test_sequences1, padding=pad_type, truncating=trunc_type, maxlen=maxlen1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Input Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1 = Input(shape=(x_train1.shape[1]))\n",
    "input_2 = Input(shape=(x_train2.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=Embedding(len(tokenizer.word_index) + 1, embedding_size)(input_1)\n",
    "v=Bidirectional(LSTM(embedding_size, return_sequences=True, dropout=.5))(v)\n",
    "v=Bidirectional(LSTM(embedding_size, return_sequences=False, dropout=.2))(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=Dense(128)(input_2)\n",
    "y=Dropout(.5)(y)\n",
    "y=Dense(64)(y)\n",
    "y=Dropout(.4)(y)\n",
    "y=Dense(32)(y)\n",
    "y=Dropout(.2)(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat=tf.keras.layers.concatenate([v,y], axis=-1)\n",
    "output=Dense(16,activation='relu')(concat)\n",
    "output=Dropout(.1)(output)\n",
    "output=Dense(8, activation='relu')(output)\n",
    "output=Dense(1, activation='sigmoid')(output)\n",
    "model=Model(inputs=[input_1, input_2], outputs=output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/GPU:0'):    \n",
    "    model = model\n",
    "    SGD= tensorflow.keras.optimizers.SGD(lr=0.0001, nesterov=True)\n",
    "    adam=tensorflow.keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(loss=tensorflow.keras.losses.binary_crossentropy, \n",
    "              optimizer=adam, metrics=['binary_accuracy'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True, expand_nested=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "reset_keras()\n",
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor='binary_accuracy', verbose=2,\n",
    "    save_best_only=True, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.load_weights('best_model.hdf5')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "lb=LabelEncoder()\n",
    "y_train=lb.fit_transform(y_train)\n",
    "reset_keras()\n",
    "\n",
    "with tf.device('/GPU:0'):    \n",
    "    history=model.fit(\n",
    "                  x=[x_train1, \n",
    "                     x_train2],y=y_train,\n",
    "                  batch_size=mybatch,verbose=1,epochs=num_epochs, callbacks=[checkpoint], validation_split=.3) \n",
    "    \n",
    "#validation_split=.1 removed after tuning models\n",
    "#2 epochs works well (hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=model.predict([x_test1, x_test2])\n",
    "predict1=np.squeeze(predict)\n",
    "np.unique(np.round(predict1,0), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
